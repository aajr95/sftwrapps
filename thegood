#I HAVE INCLUDED WORKINGS AS COMMENTS TO BETTER SHOW PROCESS

#THIS FILE CONTAINS A STANDARD VADER SENTIMENT ANALYSIS AS WELL AS A TRAINING BASED 
#METHOD THAT WAS CREADED FOR THIS ASSIGNMENT WITH NO MODULES EXCEPT PD.DATAFRAMES AS STORAGE

####Setup


import pandas as pd
from pandas import ExcelWriter
#vader sentiment analysis tool citation : Hutto, C.J. & Gilbert, E.E. (2014). VADER: A Parsimonious Rule-based Model for Sentiment Analysis of Social Media Text. Eighth International Conference on Weblogs and Social Media (ICWSM-14). Ann Arbor, MI, June 2014.
from nltk.sentiment.vader import SentimentIntensityAnalyzer #VADER
sia = SentimentIntensityAnalyzer()
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize#NATURAL LANGUAGE TOOLKIT FOR WORD CLEANING
import seaborn as sns #PRETTY PLOTS
import matplotlib.pyplot as plt
import seaborn as sns


file = 'G:\Tweets.CSV'#REFERENCE TWEET CSV LOCATION
pd.set_option('max_colwidth', 800)#displays full tweets

#reading data and storing stopwords for cleaning of data later
red_data = pd.read_csv(file)
red_data['tweet_created'] = pd.to_datetime(red_data['tweet_created'])
stopwords = stopwords.words("english")

RUNNUM = len(red_data)

#listing all airlines in original dataset to create column dataframe below, lowercasing and stripping space
airlines = list(red_data.airline.unique())
airlines1 = [x.lower() for x in airlines]
airlines2 = [x.replace(' ','') for x in airlines]
airlines3 = [x.replace(' ','') for x in airlines1]
airlines = airlines + airlines1 + airlines2 + airlines3


#THIS IS THE DATASET BEING INTERPRITED. HEAD IS USED TO SPEED UP TESTING. REMOVE '.head(number)' TO PERFORM OPERATION OVER FULL DATASET
condensed = red_data.head(RUNNUM)





####Pre-Processing for vader
#Not entirely sure about efficiency of this process however this is the method I am used to programming in. 
#All values are processed and assigned within a loop and added to the dataframe at the end of the loop. 
#The same process is used for the training approach below however there is definately a more efficient method, I am just more comfortable at this itterative method
#dataset conversion and stopword cleaning

vaderdf = pd.DataFrame()

for i in range(0, len(condensed)):
    
    #CLEANUP AND RESTRING
    tokenized = word_tokenize(str(condensed.text.loc[i]).lower())
    cleaned = [word for word in tokenized if word not in stopwords]
    filter = [word for word in cleaned
        if 'http' not in word
        and not word.startswith('@')
        and not word.startswith('#')]
    rejoined = ' '.join(cleaned)
    
    #AIRLINE CLASSIFICATION
    for x in airlines:
        if x in rejoined.split():
            ref_airline = x
            
    #VVVVVADER
    values = list(sia.polarity_scores(rejoined).values())
    
    #Discrete determination - set values for identifying positive or negative tweet as per : https://github.com/cjhutto/vaderSentiment/blob/master/README.rst#about-the-scoring
    sentiment = 'neutral'
    if values[3] >= 0.05:
        sentiment = 'postitive'
    elif values[3] <= -0.05:
        sentiment = 'negative'

    ##STORE
    vaderdf = vaderdf.append({'text': rejoined, 'neg':values[0], 'neutral':values[1], 'positive':values[2], 'comp':values[3], 'airline':ref_airline, 'Sentiment':sentiment },ignore_index=True)
 

#print(vaderdf)

#SCRAPPPP
#sentences = ["hello","this is fantastic"] #was checking vadar needed to add joined line to use properly as quotation marks were causing problems
#    workingdfneg = workingdfneg.append({'negative':values[1]},ignore_index=True)
#    workingdfneu = workingdfneu.append({'neutral':values[2]},ignore_index=True)
#    workingdfpos = workingdfpos.append({'positive':values[3]},ignore_index=True)
#    workingdfcomp = workingdcomp.append({'compound':values[4]},ignore_index=True) #ERROR WAS VALUES STARTS AT 0 NOT 1 ;_;

############################################################################################
#
#
#
#
#
#
####BELOW THIS POINT IS A TRAINING APPROACH THAT I MADE UP USING A VERY SIMPLE INTEGER VALUATION BASED ON CLASSIFICATION OF POSITIVE AND NEGATIVE IN THE ORIGINAL WITH REMOVAL OF STOPWORDS



#PARAMETERS
training_iterations = int(RUNNUM*0.15)     #ERROR MESSAGE DIDNT SHOW ME THAT THIS WAS A FLOAT AND NOT ABLE TO HALF INDEXES UNTIL IT WAS WRAPPED IN INT. WAS VERY LONG TO FIND THIS ISSUE LOL
testing_iterations = RUNNUM

#FOR TRAINING SET OF 1000 AND FULL DATASET TESTING, TIME IS : 1M 48S



#TRAINING#
tweets_train = pd.DataFrame()
tada = red_data[['airline_sentiment', 'text']]
traindata = tada.head(training_iterations)
word_map = {}
conotion_map = {}
    
for i in range(0,len(traindata)):
    toke = word_tokenize(str(traindata.text.loc[i]).lower())
    fltr = [word for word in toke
            if 'http' not in word
            and not word.startswith('@')
            and not word.startswith('#')]
    clnd = [word for word in fltr if word not in stopwords]
    rejnd = ' '.join(clnd)    
        
    fltr_airlines = [word for word in clnd #added this because airlines were being given a value and that was skewing the dataset
                     if not word in airlines]
    airline_rjn = ' '.join(fltr_airlines)
    
    for word in rejnd.split(): #MAKES FOLLOWING LOOP SIMPLER TO PLAN LOGICALLY AS WORD_MAP IS A STORE OF WORDS IN TWEETS AND FREQUENCY
        if word in word_map:
            word_map[word] = word_map[word] + 1
        else:
            word_map[word] = 1 
                  
    for word in airline_rjn.split():
        if word in word_map:
            if traindata.airline_sentiment.loc[i] == 'positive': # Classified as positive word  
                if word in conotion_map:
                    conotion_map[word] = conotion_map[word] + 1 #Adds value to word implying positive connotation (conotion short for connotation)
                else:
                    conotion_map[word] = 1
            elif traindata.airline_sentiment.loc[i] == 'negative': #Subtracts value from word implying negative connotation 
                if word in conotion_map:
                    conotion_map[word] = conotion_map[word] - 1
                else:
                    conotion_map[word] = -1
            else:
                conotion_map[word] = 0
        else:
            conotion_map[word] = 0  
        
        conotion_map[word] = conotion_map[word]/len(word_tokenize(rejnd)) #this places equal value to each tweet in an attempt to standardize dataset
                
        
                
#TESTING
    
testdata = red_data.head(testing_iterations)

for i in range(0,len(testdata)):
    toke = word_tokenize(str(testdata.text.loc[i]).lower()) #Recleaning the words
    fltr = [word for word in toke
            if 'http' not in word
            and not word.startswith('@')
            and not word.startswith('#')
            ]
    clnd = [word for word in fltr if word not in stopwords]
    rejnd = ' '.join(clnd)  
   

    sentiment_value = 0
    for element in rejnd.split():#relooping individual words for inspection
        if element in conotion_map:
            sentiment_value = sentiment_value + conotion_map[element]
        else:
            sentiment_value = sentiment_value
    
        
            
    tweets_train = tweets_train.append({'text': rejnd, 'sentiment':sentiment_value}, ignore_index = True)



#normalizing with adaptable normalization around -1 and 1 
trained_df = pd.DataFrame()

for i in range (0, len(tweets_train)):
    tweet = tweets_train.text.loc[i]
    sentiment_normalized = (((tweets_train.sentiment.loc[i] - min(list(tweets_train.sentiment)))/(max(list(tweets_train.sentiment)) - min(list(tweets_train.sentiment))))*2)-1
      
#GIVING DISCRETE VALUE BASED ON SAME VALUES FOR VADER TO GIVE ACCURATE COMPARISON  
    sentiment_discrete = 'neutral'
    if sentiment_normalized >= 0.05:
        sentiment_discrete = 'postitive'
    elif sentiment_normalized <= -0.05:
        sentiment_discrete = 'negative'

#STORING TEsTED VALUES FROM TRAINING DATA   
    trained_df = trained_df.append({'text': tweet, 'sentiment_value':sentiment_normalized, 'sentiment_discrete':sentiment_discrete}, ignore_index = True)




finaldf = pd.DataFrame()
for i in range (0, RUNNUM):
    
    orig_sent_adj = red_data.airline_sentiment_confidence.loc[i]
    if red_data.airline_sentiment.loc[i] == 'negative':
        orig_sent_adj = -orig_sent_adj
    elif red_data.airline_sentiment.loc[i] == 'neutral':
        orig_sent_adj = 0
    
    finaldf = finaldf.append({'ORIGINAL TWEET':red_data.text.loc[i],
                              'ORIGINAL_AIRLINE':red_data.airline.loc[i],
                              'VADER_AIRLINE':vaderdf.airline.loc[i],
                              'TIME':red_data.tweet_created.loc[i],
                              'ORIGINAL_SENTIMENT':red_data.airline_sentiment.loc[i],
                              'ORINGINAL_SENTVAL_ADJUSTED':orig_sent_adj, #SIMPLY TAKES CONFIDENCE AND INVERTS FOR NEGATIVE SENTIMENT AND SETS TO 0 FOR NEUTRAL                             
                              'VADER_SENTIMENT':vaderdf.Sentiment.loc[i],
                              'VADER_SENTVAL':vaderdf.comp.loc[i],
                              'TRAINED_SENTIMENT':trained_df.sentiment_discrete.loc[i],
                              'TRAINED_SENTVAL':trained_df.sentiment_value.loc[i],
                                }, ignore_index = True)

    

#writer = ExcelWriter('export.xlsx')
#finaldf.to_excel(writer, sheet_name = 'sheet2')
#writer.save()
#NOTE FILE TOO BIG IF FULL ANALYSIS IS DONE


##DATA VISUALIZATION

finaldf = pd.melt(finaldf, value_vars=['TRAINED_SENTVAL', 'VADER_SENTVAL', 'ORINGINAL_SENTVAL_ADJUSTED'], id_vars='ORIGINAL_SENTIMENT')

#%%
sns.set(style = "whitegrid")

plot1 = sns.violinplot(x='variable', y='value', hue = 'ORIGINAL_SENTIMENT', data = finaldf)
plt.legend(loc='lower left')
plt.show()
fig1 = plot1.get_figure()
fig1.savefig('15trainingviolin.png')


plot2 = sns.boxplot(x='variable', y='value', hue = 'ORIGINAL_SENTIMENT', data = finaldf)
plt.legend(loc='lower left')
plt.show()
fig2 = plot2.get_figure()
fig2.savefig('15trainingbox.png')

plot3 = sns.lvplot(x='variable', y='value', hue = 'ORIGINAL_SENTIMENT', data = finaldf)
plt.legend(loc='lower left')
plt.show()
fig3 = plot3.get_figure()
fig3.savefig('15traininglv.png')

plot4 = sns.swarmplot(x='variable', y='value', hue = 'ORIGINAL_SENTIMENT', data = finaldf)
plt.legend(loc='lower left')
plt.show()
fig4 = plot4.get_figure()
fig4.savefig('15traininglv.png')
#%%
